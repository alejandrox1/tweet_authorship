{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder                                  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.cluster.vq import whiten\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3991, 3)\n",
      "(3988, 3)\n"
     ]
    }
   ],
   "source": [
    "# INPUT                                                                                                                                     \n",
    "f_authorship = '2k_users/authorship.csv'                                                      \n",
    "                                                                                \n",
    "### PREPROCESSING                                                               \n",
    "df = pd.read_csv(f_authorship) \n",
    "print(df.shape)\n",
    "df.drop_duplicates()  \n",
    "# remove tweets with no words (just emojis)\n",
    "df = df[ [(len(word_tokenizer.tokenize(x.lower())) > 0) for x in df['text']] ]\n",
    "print(df.shape)\n",
    "np.random.seed(1)                                                               \n",
    "df = df.reindex(np.random.permutation(df.index))                                                                  \n",
    "                                                                                \n",
    "X = df.loc[:, 'text'].values                                                    \n",
    "y = df.loc[:, 'user_id'].values                                                 \n",
    "le = LabelEncoder()                                                             \n",
    "y = le.fit_transform(y)                                                         \n",
    "                                                                                \n",
    "# Train test split                                                              \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>\"I like the comfort of knowing that women are ...</td>\n",
       "      <td>304293352017379329</td>\n",
       "      <td>265463749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>Best @ATLASexperiment section ever. MT @claran...</td>\n",
       "      <td>502575966347354112</td>\n",
       "      <td>174312391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>I feel like a kid learning to walk #StringTheo...</td>\n",
       "      <td>378295513910902784</td>\n",
       "      <td>265463749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text                  id  \\\n",
       "187   \"I like the comfort of knowing that women are ...  304293352017379329   \n",
       "3080  Best @ATLASexperiment section ever. MT @claran...  502575966347354112   \n",
       "1811  I feel like a kid learning to walk #StringTheo...  378295513910902784   \n",
       "\n",
       "        user_id  \n",
       "187   265463749  \n",
       "3080  174312391  \n",
       "1811  265463749  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicanl Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create feature vectors\n",
    "num_docs = X_train.shape[0]\n",
    "\n",
    "# words/sentence avg., words/sentence std., unique words/total words\n",
    "# commas/semicolons/periods per sentence\n",
    "fvs_punct_lexical = np.zeros((num_docs, 6), np.float64)\n",
    "for i, doc in enumerate(X_train):  \n",
    "    # PUNCTUATION FEATURES\n",
    "    # note: the nltk.word_tokenize includes punctuation\n",
    "    tokens = nltk.word_tokenize(doc)           ###\n",
    "    # LEXICAL FEATURES\n",
    "    words = word_tokenizer.tokenize(doc)       ###\n",
    "    sentences = sentence_tokenizer.tokenize(doc)\n",
    "    vocab = set(words)\n",
    "    words_per_sentence = np.array([len(word_tokenizer.tokenize(s)) for s in sentences])\n",
    " \n",
    "    # Commas per sentence\n",
    "    fvs_punct_lexical[i, 0] = tokens.count(',') / float(len(sentences))\n",
    "    # Semicolons per sentence\n",
    "    fvs_punct_lexical[i, 1] = tokens.count(';') / float(len(sentences))\n",
    "    # Colons per sentence\n",
    "    fvs_punct_lexical[i, 2] = tokens.count(':') / float(len(sentences))\n",
    "    \n",
    "    # average number of words per sentence\n",
    "    fvs_punct_lexical[i, 3] = words_per_sentence.mean()\n",
    "    # sentence length variation\n",
    "    fvs_punct_lexical[i, 4] = words_per_sentence.std()\n",
    "    # Lexical diversity - proportion of unique words\n",
    "    fvs_punct_lexical[i, 5] = len(vocab) / float(len(words))\n",
    "    \n",
    "    \n",
    "# apply whitening to decorrelate the features\n",
    "#fvs_punct_lexical = whiten(fvs_lexical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get most common words in the whole book\n",
    "NUM_TOP_WORDS = 1000\n",
    "all_text = ' '.join(X_train)\n",
    "all_tokens = nltk.word_tokenize(all_text)\n",
    "fdist = nltk.FreqDist(all_tokens)\n",
    "vocab = fdist.most_common(NUM_TOP_WORDS)\n",
    "\n",
    "# use sklearn to create the bag for words feature vector for each chapter\n",
    "tweet_token = TweetTokenizer()\n",
    "vectorizer = CountVectorizer(vocabulary=vocab, \n",
    "                             tokenizer=tweet_token.tokenize)\n",
    "fvs_bow = vectorizer.fit_transform(X_train).toarray().astype(np.float64)\n",
    "\n",
    "# normalise by rows\n",
    "#from sklearn.preprocessing import normalize\n",
    "#fvs_bow = normalize(fvs_bow, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntatic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get part of speech for each token in each chapter\n",
    "def token_to_pos(doc):\n",
    "    tweet_token = TweetTokenizer()\n",
    "    tokens = tweet_token.tokenize(text)\n",
    "    return [tag for word, tag in nltk.pos_tag(tokens)]\n",
    "\n",
    "docs_pos = [token_to_pos(doc) for doc in X_train]\n",
    "\n",
    "# count frequencies for common POS types\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "poss = [ item for sublist in docs_pos for item in sublist ]\n",
    "pos_dist = Counter(poss)\n",
    "pos_list = [ pos for pos, count in pos_dist.most_common() ]\n",
    "\n",
    "fvs_syntax = np.array([ [doc.count(pos) for pos in pos_list] for doc in docs_pos]).astype(np.float64)\n",
    " \n",
    "# normalise by dividing each row by number of tokens in the chapter\n",
    "# fvs_syntax /= np.c_[np.array([len(doc) for doc in docs_pos])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PredictAuthors(fvs, X_train, y_train):\n",
    "    sc = StandardScaler()\n",
    "    fvs_std = sc.fit_transform(fvs)\n",
    "    \n",
    "    km = KMeans(n_clusters=2, \n",
    "                init='k-means++', \n",
    "                n_init=300, \n",
    "                max_iter=1000,\n",
    "                tol=1e-6,\n",
    "                verbose=0)\n",
    "    km.fit_predict(fvs_std)\n",
    "\n",
    "    c_docs = defaultdict(list)\n",
    "    c_ids = defaultdict(list)\n",
    "    # organize documents into their clusters\n",
    "    for i, label in enumerate(km.labels_):\n",
    "        c_docs[label].append(X_train[i])\n",
    "        c_ids[label].append(y_train[i])\n",
    "    # get distribution of documents for each cluster\n",
    "    for label in c_ids.keys():\n",
    "        c_ids[label] = Counter(c_ids[label])\n",
    "    \n",
    "    return km, c_docs, c_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1465), (0, 1149)]\n",
      "[(0, 845), (1, 529)]\n",
      "---------------------------------------------------- Cluster 1\n",
      "Graduate school is way better than undergrad!\n",
      "@Lucretius21c @TariqYasmine It seems that @AstroKatie was missed in the making of this list. :(\n",
      "@xoxosussyxoxo lol thanks but I don't get no money for that\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------- Cluster 0\n",
      "The Louis Lyons #arXiv paper that @jonmbutterworth mentioned in his post on #statistics in particle #physics: http://t.co/gCEN9QQuHP\n",
      "@AnElizardbreath Welcome to Twitter. It takes some building and getting used to, but is extremely useful.\n",
      "@roseveleth That people think they actually have an opinion on things they know nothing about confuses me. They just have nothing.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "km, c_docs, c_ids = PredictAuthors(fvs_punct_lexical, X_train, y_train) \n",
    "\n",
    "for label in c_ids.keys():\n",
    "    print(c_ids[label].most_common())\n",
    "for label, tweet in c_docs.items():\n",
    "    print('---------------------------------------------------- Cluster {}'.format(label))\n",
    "    for text in tweet[:3]:\n",
    "        print(text)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1994), (0, 1994)]\n",
      "---------------------------------------------------- Cluster 0\n",
      "Graduate school is way better than undergrad!\n",
      "The Louis Lyons #arXiv paper that @jonmbutterworth mentioned in his post on #statistics in particle #physics: http://t.co/gCEN9QQuHP\n",
      "@AnElizardbreath Welcome to Twitter. It takes some building and getting used to, but is extremely useful.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "km, c_docs, c_ids = PredictAuthors(fvs_bow, X_train, y_train) \n",
    "\n",
    "for label in c_ids.keys():\n",
    "    print(c_ids[label].most_common())\n",
    "for label, tweet in c_docs.items():\n",
    "    print('---------------------------------------------------- Cluster {}'.format(label))\n",
    "    for text in tweet[:3]:\n",
    "        print(text)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1994), (0, 1994)]\n",
      "---------------------------------------------------- Cluster 0\n",
      "Graduate school is way better than undergrad!\n",
      "The Louis Lyons #arXiv paper that @jonmbutterworth mentioned in his post on #statistics in particle #physics: http://t.co/gCEN9QQuHP\n",
      "@AnElizardbreath Welcome to Twitter. It takes some building and getting used to, but is extremely useful.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "km, c_docs, c_ids = PredictAuthors(fvs_syntax, X_train, y_train) \n",
    "\n",
    "for label in c_ids.keys():\n",
    "    print(c_ids[label].most_common())\n",
    "for label, tweet in c_docs.items():\n",
    "    print('---------------------------------------------------- Cluster {}'.format(label))\n",
    "    for text in tweet[:3]:\n",
    "        print(text)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
