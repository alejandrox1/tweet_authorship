{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "dest = os.path.join('pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "    print(dest)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from tweepy import API\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Cursor\n",
    "from tweepy import TweepError\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException \n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpter Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Authetication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_twitter_auth():\n",
    "    \"\"\"Setup Twitter Authentication.\n",
    "    \n",
    "    Return: tweepy.OAuthHandler object\n",
    "    \"\"\"\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return auth\n",
    "    \n",
    "def get_twitter_client():\n",
    "    \"\"\"Setup Twitter API Client.\n",
    "    \n",
    "    Return: tweepy.API object\n",
    "    \"\"\"\n",
    "    auth = get_twitter_auth()\n",
    "    client = API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makedir(screen_name):\n",
    "    \"\"\"Create subdirectory 'users/screen_name' to store mined data.\n",
    "    \n",
    "    Params\n",
    "    -------\n",
    "    screen_name : str\n",
    "    \"\"\"\n",
    "    dirname = 'users/{}'.format(screen_name)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(dirname, mode=0o755, exist_ok=True)\n",
    "    except OSError:\n",
    "        print('Directory {} already exists.'.format(dirname))\n",
    "    except Exception as e:\n",
    "        print('Error while creating directory {}'.format(dirname))\n",
    "        print(e)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_url(screen_name, no_rt, start, end):\n",
    "    \"\"\"Form url to access tweets via Twitter's search page.\n",
    "    Params\n",
    "    -------\n",
    "    screen_name : str\n",
    "    no_rt : bool\n",
    "    start : datetime-onj\n",
    "    end : datetime-obj\n",
    "    \n",
    "    Return: string\n",
    "    \"\"\"\n",
    "    url1 = 'https://twitter.com/search?f=tweets&q=from%3A'\n",
    "    url2 = screen_name + '%20since%3A' + start.strftime('%Y-%m-%d') \n",
    "    url3 = ''\n",
    "    if no_rt:\n",
    "        url3 = '%20until%3A' + end.strftime('%Y-%m-%d') + '%20&src=typd'\n",
    "    else:\n",
    "        url3 = '%20until%3A' + end.strftime('%Y-%m-%d') + '%20include%3Aretweets&src=typd'\n",
    "    \n",
    "    return url1 + url2 + url3\n",
    "    \n",
    "def increment_day(date, i):\n",
    "    \"\"\"Increment day object by i days.\n",
    "    \n",
    "    Params\n",
    "    -------\n",
    "    date : datetime-obj\n",
    "    i : int\n",
    "    \n",
    "    Return: datetime object\n",
    "    \"\"\"\n",
    "    return date + datetime.timedelta(days=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Tweets for Given User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_tweets(screen_name, no_rt=True):\n",
    "    \"\"\"Get tweets for a given user (3,200 limit)\n",
    "    \n",
    "    Create a subdir named 'users'.\n",
    "    In this subdir, a jsonl file will store all the tweets writen\n",
    "    by the given user.\n",
    "    \n",
    "    Params\n",
    "    -------\n",
    "    screen_name : str    \n",
    "    \"\"\"\n",
    "    # Make dir structure\n",
    "    makedir(screen_name)\n",
    "\n",
    "    total_tweets = 0\n",
    "    fname = 'users/{0}/usr_timeline_{0}.jsonl'.format(screen_name)\n",
    "    with open(fname, 'a') as f:\n",
    "        for page in Cursor(client.user_timeline, screen_name=screen_name, count=200).pages(4): #16): \n",
    "            for tweet in page:\n",
    "                total_tweets += 1\n",
    "                if no_rt:\n",
    "                    if not tweet.retweeted and 'RT @' not in tweet.text:\n",
    "                        f.write(json.dumps(tweet._json)+'\\n')\n",
    "                else:\n",
    "                    f.write(json.dumps(tweet._json)+'\\n')\n",
    "    return total_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_user_tweets(screen_name, start, end, no_rt=True):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "    screen_name : str\n",
    "    start : datetime-obj\n",
    "    end : datetime-obj\n",
    "    no_rt : bool\n",
    "    \n",
    "    \"\"\"\n",
    "    # Special parameters\n",
    "    fname_tweet_ids = 'users/{0}/usr_tweetids_{0}.jsonl'.format(screen_name)\n",
    "    \n",
    "    # Make dir structure\n",
    "    makedir(screen_name)\n",
    "    \n",
    "    # Selenium parames\n",
    "    delay = 1  # time to wait on each page load before reading the page\n",
    "    driver = webdriver.Chrome() \n",
    "    tweet_selector = 'li.js-stream-item'\n",
    "    id_selector = '.time a.tweet-timestamp'\n",
    "    \n",
    "    ids_total = 0\n",
    "    for day in range((end - start).days + 1):\n",
    "        # Get Twitter search url\n",
    "        startDate = increment_day(start, 0)\n",
    "        endDate = increment_day(start, 1)\n",
    "        url = twitter_url(screen_name, no_rt, startDate, endDate)\n",
    "\n",
    "        driver.get(url)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        try:\n",
    "            found_tweets = driver.find_elements_by_css_selector(tweet_selector)\n",
    "            increment = 10\n",
    "\n",
    "            # Scroll through the Twitter search page\n",
    "            while len(found_tweets) >= increment:\n",
    "                print('scrolling down to load more tweets')\n",
    "                driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "                time.sleep(delay)\n",
    "                found_tweets = driver.find_elements_by_css_selector(tweet_selector)\n",
    "                increment += 10\n",
    "\n",
    "            # Get the IDs for all Tweets\n",
    "            ids = []\n",
    "            with open(fname_tweet_ids, 'a') as fout:\n",
    "                for tweet in found_tweets:\n",
    "                    try:\n",
    "                        tweet_id = tweet.find_element_by_css_selector(\n",
    "                                    id_selector).get_attribute('href').split('/')[-1]\n",
    "                        ids.append(tweet_id)\n",
    "                        ids_total += 1\n",
    "                    except StaleElementReferenceException as e:\n",
    "                        print('lost element reference', tweet)\n",
    "                        \n",
    "                # Save ids to file\n",
    "                data_to_write = list(set(ids))\n",
    "                fout.write(json.dumps(data_to_write)+'\\n')\n",
    "            print('{} tweets found, {} total'.format(len(found_tweets), ids_total))\n",
    "        \n",
    "        except NoSuchElementException:\n",
    "            print('no tweets on this day')\n",
    "\n",
    "        start = increment_day(start, 1)\n",
    "    \n",
    "    # Close selenium driver\n",
    "    driver.close()\n",
    "    \n",
    "    return ids_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obatining the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from config import *\n",
    "client = get_twitter_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mine Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "screen_names = ['AP', 'FoxNews', 'nytimes']\n",
    "small_batch = True\n",
    "start = datetime.datetime(2017, 1, 10)  \n",
    "end = datetime.datetime(2017, 1, 16)    \n",
    "fname_tweet_ids = 'all_ids.json'\n",
    "total_tweets = []\n",
    "\n",
    "if small_batch:\n",
    "    for screen_name in screen_names:\n",
    "        num_tweets = get_user_tweets(screen_name)\n",
    "        total_tweets.append(num_tweets)\n",
    "else: \n",
    "    for screen_name in screen_names:\n",
    "        num_tweets = get_all_user_tweets(screen_name, start, end, no_rt=True)\n",
    "        total_tweets.append(num_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800, 800, 800]\n"
     ]
    }
   ],
   "source": [
    "print(total_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[#######################       ] | ETA: 00:00:02 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:01 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00 | ETA: 00:00:00"
     ]
    }
   ],
   "source": [
    "f_authorship = 'users/authorship.csv'\n",
    "\n",
    "pbar = pyprind.ProgBar(sum(total_tweets))\n",
    "with open(f_authorship, 'w') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    # Header\n",
    "    writer.writerow(['text','id','user_id'])\n",
    "\n",
    "    for screen_name in screen_names:\n",
    "        if small_batch:\n",
    "            fin = 'users/{0}/usr_timeline_{0}.jsonl'.format(screen_name)\n",
    "            with open(fin, 'r') as f:\n",
    "                for line in f:\n",
    "                    tweet = json.loads(line)\n",
    "                    writer.writerow([tweet['text'], tweet['id'], tweet['user']['id']])\n",
    "                    #fout.write('\"{0}\",{1},{2}\\n'.format(tweet['text'].encode(\"utf-8\"), tweet['id'], tweet['user']['id']))\n",
    "                    pbar.update()\n",
    "        else:\n",
    "            fin = 'users/{0}/usr_tweetids_{0}.jsonl'.format(screen_name)\n",
    "            with open(fin, 'r') as f:\n",
    "                for line in f:\n",
    "                    ids = json.loads(line)\n",
    "                    \n",
    "                    for tweetId in ids:\n",
    "                        tweet = client.get_status(tweetId)\n",
    "                        writer.writerow([tweet.text, tweet.id, tweet.user.id])\n",
    "                        #fout.write('\"{0}\",{1},{2}\\n'.format(tweet.text.encode(\"utf-8\"), tweet.id, tweet.user.id))\n",
    "                        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>Alex Jones \"is apparently taking on a new role...</td>\n",
       "      <td>833871892150964224</td>\n",
       "      <td>807095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>Trump escalated his attack on Sweden, but gove...</td>\n",
       "      <td>833870631494098944</td>\n",
       "      <td>807095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>Inside the brutal world of comedy open mikes h...</td>\n",
       "      <td>833863436677283840</td>\n",
       "      <td>807095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text                  id  \\\n",
       "1883  Alex Jones \"is apparently taking on a new role...  833871892150964224   \n",
       "1884  Trump escalated his attack on Sweden, but gove...  833870631494098944   \n",
       "1885  Inside the brutal world of comedy open mikes h...  833863436677283840   \n",
       "\n",
       "      user_id  \n",
       "1883   807095  \n",
       "1884   807095  \n",
       "1885   807095  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_authorship = 'users/authorship.csv'\n",
    "df = dd.read_csv(f_authorship)\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1886\n",
      "1886\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df.drop_duplicates()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1886, 3)\n",
      "(1886, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forty-three ex-congregants at NC church speak ...</td>\n",
       "      <td>836290574604972032</td>\n",
       "      <td>51241574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BREAKING: Attorneys: Trump administration says...</td>\n",
       "      <td>836276697091670016</td>\n",
       "      <td>51241574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP INVESTIGATION: Dozens of ex-congregants des...</td>\n",
       "      <td>836259644981067776</td>\n",
       "      <td>51241574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                  id  \\\n",
       "0  Forty-three ex-congregants at NC church speak ...  836290574604972032   \n",
       "1  BREAKING: Attorneys: Trump administration says...  836276697091670016   \n",
       "2  AP INVESTIGATION: Dozens of ex-congregants des...  836259644981067776   \n",
       "\n",
       "    user_id  \n",
       "0  51241574  \n",
       "1  51241574  \n",
       "2  51241574  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_authorship = 'users/authorship.csv'\n",
    "df = pd.read_csv(f_authorship)\n",
    "print(df.shape)\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data -> Prprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def preprocessor(text):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text) + ' '.join(emoticons).replace('-', '')\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "tweet_token = TweetTokenizer()\n",
    "def tokenizer_twitter(text):\n",
    "    return tweet_token.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "#df['text'] = df['text'].apply(preprocessor)\n",
    "\n",
    "X = df.loc[:, 'text'].values\n",
    "y = df.loc[:, 'user_id'].values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(np.unique(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, \n",
    "                           param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Best parameter set: {} \\n'.format(gs_lr_tfidf.best_params_))\n",
    "print('CV Accuracy: {:.3f}'.format(gs_lr_tfidf.best_score_))\n",
    "clf_lr = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: {:.3f}'.format(clf_lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores =\\\n",
    "                learning_curve(estimator=clf_lr,\n",
    "                               X=X_train,\n",
    "                               y=y_train,\n",
    "                               train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                               cv=10,\n",
    "                               n_jobs=-1)\n",
    "    \n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "#plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/learning_curve.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = clf_lr.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "\n",
    "plt.xlabel('predicted label', fontsize=15)\n",
    "plt.ylabel('true label', fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/confusion_matrix.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', SVC(random_state=1))])\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],\n",
    "               'clf__C': param_range, \n",
    "               'clf__kernel': ['linear']},\n",
    "              {'vect__ngram_range': [(1, 3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__C': param_range, \n",
    "               'clf__kernel': ['linear']},\n",
    "              {'vect__ngram_range': [(1, 3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],\n",
    "               'clf__C': param_range, \n",
    "               'clf__gamma': param_range, \n",
    "               'clf__kernel': ['rbf']},\n",
    "              {'vect__ngram_range': [(1, 3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__C': param_range, \n",
    "               'clf__gamma': param_range, \n",
    "               'clf__kernel': ['rbf']}]\n",
    "\n",
    "gs_svc_tfidf = GridSearchCV(estimator=svc_tfidf, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  cv=5,\n",
    "                  verbose=1,\n",
    "                  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_svc_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Best parameter set: {} \\n'.format(gs_svc_tfidf.best_params_))\n",
    "print('CV Accuracy: {:.3f}'.format(gs_svc_tfidf.best_score_))\n",
    "clf_svc = gs_svc_tfidf.best_estimator_\n",
    "print('Test Accuracy: {:.3f}'.format(clf_svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_tfidf = Pipeline([('vect', tfidf),\n",
    "                    ('clf', MultinomialNB())])\n",
    "\n",
    "param_range = [0.25, 0.5, 0.75, 1.0]\n",
    "param_grid = [{'vect__ngram_range': [(1, 3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],\n",
    "               'clf__alpha': param_range},\n",
    "              {'vect__ngram_range': [(1, 3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__alpha': param_range}]\n",
    "                   \n",
    "gs_nb_tfidf = GridSearchCV(estimator=svc_tfidf, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  cv=5,\n",
    "                  verbose=1,\n",
    "                  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_nb_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Best parameter set: {} \\n'.format(gs_nb_tfidf.best_params_))\n",
    "print('CV Accuracy: {:.3f}'.format(gs_nb_tfidf.best_score_))\n",
    "clf_nb = gs_nb_tfidf.best_estimator_\n",
    "print('Test Accuracy: {:.3f}'.format(clf_nb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', SGDClassifier(random_state=42)),])\n",
    "\n",
    "param_range = [0.25, 0.5, 0.75, 1.0]\n",
    "param_grid = [{'vect__ngram_range': [(1, 3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],\n",
    "               'clf__loss' : ['hinge', 'log'],\n",
    "               'clf__penalty' : ['l1', 'l2'],\n",
    "               'clf__n_iter' : [3,5,7],\n",
    "               'clf__alpha': param_range},\n",
    "              {'vect__ngram_range': [(1, 3)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__loss' : ['hinge', 'log'],\n",
    "               'clf__penalty' : ['l1', 'l2'],\n",
    "               'clf__n_iter' : [3,5,7],\n",
    "               'clf__alpha': param_range}]\n",
    "\n",
    "gs_sgd_tfidf = GridSearchCV(estimator=sgd_tfidf, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  cv=5,\n",
    "                  verbose=1,\n",
    "                  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_sgd_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Best parameter set: {} \\n'.format(gs_sgd_tfidf.best_params_))\n",
    "print('CV Accuracy: {:.3f}'.format(gs_sgd_tfidf.best_score_))\n",
    "clf_sgd = gs_sgd_tfidf.best_estimator_\n",
    "print('Test Accuracy: {:.3f}'.format(clf_sgd.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(gs_lr_tfidf, open(os.path.join(dest, 'gs_logit.pkl'), 'wb'), protocol=4)\n",
    "pickle.dump(gs_svc_tfidf, open(os.path.join(dest, 'gs_svc.pkl'), 'wb'), protocol=4)\n",
    "pickle.dump(gs_nb_tfidf, open(os.path.join(dest, 'gs_nb.pkl'), 'wb'), protocol=4)\n",
    "pickle.dump(gs_sgd_tfidf, open(os.path.join(dest, 'gs_sgd.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = pickle.load(open(os.path.join(dest, 'gs_logit.pkl'), 'rb'))\n",
    "clf2 = pickle.load(open(os.path.join(dest, 'gs_svc.pkl'), 'rb'))\n",
    "clf3 = pickle.load(open(os.path.join(dest, 'gs_nb.pkl'), 'rb'))\n",
    "clf4 = pickle.load(open(os.path.join(dest, 'gs_sgd.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0a5a5a0ec8bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best parameter set: {} \\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CV Accuracy: {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Accuracy: {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: {} \\n'.format(clf1.best_params_))\n",
    "print('CV Accuracy: {:.3f}'.format(clf1.best_score_))\n",
    "print('Test Accuracy: {:.3f}'.format(clf1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(gs_lr_tfidf, open(os.path.join(dest, 'trial.pkl'), 'wb'), protocol=4)\n",
    "clf0 = pickle.load(open(os.path.join(dest, 'trial.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 100.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__norm': None, 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_twitter at 0x7f9261ff9158>, 'vect__use_idf': False} \n",
      "\n",
      "CV Accuracy: 0.872\n",
      "Test Accuracy: 0.874\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: {} \\n'.format(clf0.best_params_))\n",
    "print('CV Accuracy: {:.3f}'.format(clf0.best_score_))\n",
    "print('Test Accuracy: {:.3f}'.format(gs_lr_tfidf.best_estimator_.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
